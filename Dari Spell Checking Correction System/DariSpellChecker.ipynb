{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "from parsivar import Normalizer\n",
    "from parsivar import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalize = Normalizer()\n",
    "Tokenize = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 968534 total words in the corpus\n"
     ]
    }
   ],
   "source": [
    "def read_corpus(fileName):\n",
    "    with open(fileName, \"r\",encoding = 'UTF-8') as file:\n",
    "        lines = file.readlines()\n",
    "        words = []\n",
    "        \n",
    "        for line in lines:\n",
    "            words += re.findall(r'\\w+', line)\n",
    "        return words\n",
    "            \n",
    "            \n",
    "corpus = read_corpus(\"./Dictionary.txt\")\n",
    "print(f'There are {len(corpus )} total words in the corpus')\n",
    "#برای دیدن مجموعه لغات در داخل دیکشنری ازین رویش استفاده میکنیم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 228042 unique wordsin dec\n"
     ]
    }
   ],
   "source": [
    "#اگر بخواهیم تنها لفت های یونیک باشد پس از این روش کار میگیریم\n",
    "vocaps = set(corpus)\n",
    "print(f\"there are {len (vocaps)} unique wordsin dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dari_alphabets = 'ا ب پ ت ث ج چ ح خ د ذ ر ز  س ش ض  ع غ ف ق ک گ ل م ن و ه ی ء'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_one_edits(word):\n",
    "    spliting_operation = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    swap = [left + right[1] + right[0] + right[2:] for left, right in spliting_operation if len(right)>1]\n",
    "    deletes = [left + right[1:] for left, right in spliting_operation if right]\n",
    "    replaces = [left + c + right[1:] for left, right in spliting_operation for c in Dari_alphabets if right]\n",
    "    inserts = [left + c + right for left, right in spliting_operation for c in Dari_alphabets]\n",
    "    return set( swap + deletes + replaces + inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_two_edits(word):\n",
    "    return set(e2 for e1 in level_one_edits(word) for e2 in level_one_edits(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    return set(w for w in words if w in corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "List = []\n",
    "def correction(Text):\n",
    "    words = (Normalize.normalize(Text))\n",
    "    token = Tokenize.tokenize_words(words)\n",
    "    for token in words:\n",
    "        if token not in corpus:\n",
    "            candidates = known(level_one_edits(word)) or level_two_edits(word)#Note\n",
    "            n_best_select = set(sorted(candidates)[:20])\n",
    "            n_best_select = list(n_best_select)\n",
    "            List.append((f\" لغت درست: {word} :{n_best_select} است. \"))\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "متن تان را وارد نمایید :خاهم\n"
     ]
    }
   ],
   "source": [
    "word = input(\"متن تان را وارد نمایید :\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = correction(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" لغت درست: خاهم :['شاهم', 'خاکم', 'خادم', 'خانهم', 'خالم', 'خاتم', 'خواهم', 'اهم', 'خامهم', 'خامه', 'خفهم', 'خارم', 'خزهم', 'راهم', 'جاهم', 'خاشم', 'خالهم', 'خام', 'باهم', 'خانم'] است. \", \" لغت درست: خاهم :['شاهم', 'خاکم', 'خادم', 'خانهم', 'خالم', 'خاتم', 'خواهم', 'اهم', 'خامهم', 'خامه', 'خفهم', 'خارم', 'خزهم', 'راهم', 'جاهم', 'خاشم', 'خالهم', 'خام', 'باهم', 'خانم'] است. \", \" لغت درست: خاهم :['شاهم', 'خاکم', 'خادم', 'خانهم', 'خالم', 'خاتم', 'خواهم', 'اهم', 'خامهم', 'خامه', 'خفهم', 'خارم', 'خزهم', 'راهم', 'جاهم', 'خاشم', 'خالهم', 'خام', 'باهم', 'خانم'] است. \"]\n"
     ]
    }
   ],
   "source": [
    "print(List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
